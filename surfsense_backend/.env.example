DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/surfsense

#Celery Config
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
# Optional: isolate queues when sharing Redis with other apps
CELERY_TASK_DEFAULT_QUEUE=surfsense

# Redis for app-level features (heartbeats, podcast markers)
# Defaults to CELERY_BROKER_URL when not set
REDIS_APP_URL=redis://localhost:6379/0

#Electric(for migrations only)
ELECTRIC_DB_USER=electric
ELECTRIC_DB_PASSWORD=electric_password

# Periodic task interval
# # Run every minute (default)
# SCHEDULE_CHECKER_INTERVAL=1m

# # Run every 5 minutes
# SCHEDULE_CHECKER_INTERVAL=5m

# # Run every 10 minutes
# SCHEDULE_CHECKER_INTERVAL=10m

# # Run every hour
# SCHEDULE_CHECKER_INTERVAL=1h

# # Run every 2 hours
# SCHEDULE_CHECKER_INTERVAL=2h
SCHEDULE_CHECKER_INTERVAL=5m

SECRET_KEY=SECRET
NEXT_FRONTEND_URL=http://localhost:3000

# Backend URL for OAuth callbacks (optional, set when behind reverse proxy with HTTPS)
# BACKEND_URL=https://api.yourdomain.com

# Auth
AUTH_TYPE=GOOGLE or LOCAL
REGISTRATION_ENABLED=TRUE or FALSE

# Anonymous access / public chat
ANON_ACCESS_ENABLED=TRUE
ANON_SESSION_TTL_SECONDS=86400
ANON_CHAT_RATE_LIMIT_MAX_REQUESTS=20
ANON_CHAT_RATE_LIMIT_WINDOW_SECONDS=60
ANON_CHAT_MAX_HISTORY_MESSAGES=10
ANON_CHAT_DEFAULT_LLM_ID=-1
ANON_CHAT_TEMPERATURE=0.2
ANON_CHAT_RECURSION_LIMIT=40
ANON_CHAT_ENABLED_TOOLS=link_preview,display_image,scrape_webpage,search_web,smhi_weather,smhi_vaderprognoser_metfcst,smhi_vaderprognoser_snow1g,smhi_vaderanalyser_mesan2g,smhi_vaderobservationer_metobs,smhi_hydrologi_hydroobs,smhi_hydrologi_pthbv,smhi_oceanografi_ocobs,smhi_brandrisk_fwif,smhi_brandrisk_fwia,smhi_solstralning_strang,trafiklab_route,libris_search,jobad_links_search

# Optional: platform-wide default runtime flags for all chat runs (new + regenerate).
# Request-level runtime_hitl still overrides these values when provided.
#
# subagent_max_concurrency controls how many sub-agents run in parallel.
# Default: 3 (safe for cloud APIs with rate limits).
# Recommended for local vLLM: 10–20 (vLLM's continuous batching needs concurrent
# requests to saturate the GPU; too few requests = low GPU utilization).
#
# ONESEEK_RUNTIME_HITL_DEFAULT_JSON={"enabled":true,"hybrid_mode":true,"speculative_enabled":true,"subagent_enabled":true,"subagent_isolation_enabled":true,"subagent_max_concurrency":3,"subagent_context_max_chars":1400,"subagent_result_max_chars":1000,"subagent_sandbox_scope":"subagent","artifact_offload_enabled":true,"artifact_offload_storage_mode":"auto","artifact_offload_threshold_chars":4000,"context_compaction_enabled":true,"context_compaction_trigger_ratio":0.65,"cross_session_memory_enabled":true,"cross_session_memory_max_items":6,"sandbox_enabled":true,"sandbox_mode":"provisioner","sandbox_provisioner_url":"http://127.0.0.1:8002","sandbox_state_store":"file","sandbox_idle_timeout_seconds":900}
#
# vLLM-optimized variant (higher concurrency, larger max_tokens budget):
# ONESEEK_RUNTIME_HITL_DEFAULT_JSON={"enabled":true,"hybrid_mode":true,"speculative_enabled":true,"subagent_enabled":true,"subagent_isolation_enabled":true,"subagent_max_concurrency":12,"subagent_context_max_chars":1400,"subagent_result_max_chars":1000,"subagent_sandbox_scope":"subagent","artifact_offload_enabled":true,"artifact_offload_storage_mode":"auto","artifact_offload_threshold_chars":4000,"context_compaction_enabled":true,"context_compaction_trigger_ratio":0.65,"cross_session_memory_enabled":true,"cross_session_memory_max_items":6,"sandbox_enabled":true,"sandbox_mode":"provisioner","sandbox_provisioner_url":"http://127.0.0.1:8002","sandbox_state_store":"file","sandbox_idle_timeout_seconds":900}

# Public tool API keys (global)
PUBLIC_TAVILY_API_KEY=your_public_tavily_key_here
PUBLIC_WEB_SEARCH_MAX_RESULTS=5
TRAFIKLAB_API_KEY=your_trafiklab_api_key_here
GEOCODING_USER_AGENT=SurfSense/1.0 (+https://surfsense.ai)
JOBAD_LINKS_BASE_URL=https://links.api.jobtechdev.se
JOBAD_LINKS_API_KEY=

# Geoapify Static Maps API
GEOAPIFY_API_KEY=

# For Google Auth Only
GOOGLE_OAUTH_CLIENT_ID=924507538m
GOOGLE_OAUTH_CLIENT_SECRET=GOCSV

# Google Connector Specific Configurations
GOOGLE_CALENDAR_REDIRECT_URI=http://localhost:8000/api/v1/auth/google/calendar/connector/callback
GOOGLE_GMAIL_REDIRECT_URI=http://localhost:8000/api/v1/auth/google/gmail/connector/callback
GOOGLE_DRIVE_REDIRECT_URI=http://localhost:8000/api/v1/auth/google/drive/connector/callback

# Aitable OAuth Configuration
AIRTABLE_CLIENT_ID=your_airtable_client_id_here
AIRTABLE_CLIENT_SECRET=your_airtable_client_secret_here
AIRTABLE_REDIRECT_URI=http://localhost:8000/api/v1/auth/airtable/connector/callback

# ClickUp OAuth Configuration
CLICKUP_CLIENT_ID=your_clickup_client_id_here
CLICKUP_CLIENT_SECRET=your_clickup_client_secret_here
CLICKUP_REDIRECT_URI=http://localhost:8000/api/v1/auth/clickup/connector/callback

# Discord OAuth Configuration
DISCORD_CLIENT_ID=your_discord_client_id_here
DISCORD_CLIENT_SECRET=your_discord_client_secret_here
DISCORD_REDIRECT_URI=http://localhost:8000/api/v1/auth/discord/connector/callback
DISCORD_BOT_TOKEN=your_bot_token_from_developer_portal

# Atlassian OAuth Configuration
ATLASSIAN_CLIENT_ID=your_atlassian_client_id_here
ATLASSIAN_CLIENT_SECRET=your_atlassian_client_secret_here
JIRA_REDIRECT_URI=http://localhost:8000/api/v1/auth/jira/connector/callback
CONFLUENCE_REDIRECT_URI=http://localhost:8000/api/v1/auth/confluence/connector/callback

# Linear OAuth Configuration
LINEAR_CLIENT_ID=your_linear_client_id_here
LINEAR_CLIENT_SECRET=your_linear_client_secret_here
LINEAR_REDIRECT_URI=http://localhost:8000/api/v1/auth/linear/connector/callback

# Notion OAuth Configuration
NOTION_CLIENT_ID=your_notion_client_id_here
NOTION_CLIENT_SECRET=your_notion_client_secret_here
NOTION_REDIRECT_URI=http://localhost:8000/api/v1/auth/notion/connector/callback

# Slack OAuth Configuration
SLACK_CLIENT_ID=your_slack_client_id_here
SLACK_CLIENT_SECRET=your_slack_client_secret_here
SLACK_REDIRECT_URI=http://localhost:8000/api/v1/auth/slack/connector/callback

# Teams OAuth Configuration
TEAMS_CLIENT_ID=your_teams_client_id_here
TEAMS_CLIENT_SECRET=your_teams_client_secret_here
TEAMS_REDIRECT_URI=http://localhost:8000/api/v1/auth/teams/connector/callback

#Composio Coonnector
COMPOSIO_API_KEY=your_api_key_here
COMPOSIO_ENABLED=TRUE
COMPOSIO_REDIRECT_URI=http://localhost:8000/api/v1/auth/composio/connector/callback

# Embedding Model
# Examples:
#     # Get sentence transformers embeddings
#     embeddings = AutoEmbeddings.get_embeddings("sentence-transformers/all-MiniLM-L6-v2")

#     # Get OpenAI embeddings
#     embeddings = AutoEmbeddings.get_embeddings("openai://text-embedding-ada-002", api_key="...")

#     # Get Anthropic embeddings
#     embeddings = AutoEmbeddings.get_embeddings("anthropic://claude-v1", api_key="...")

#     # Get Cohere embeddings
#     embeddings = AutoEmbeddings.get_embeddings("cohere://embed-english-light-v3.0", api_key="...")
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Rerankers Config
RERANKERS_ENABLED=TRUE or FALSE(Default: FALSE)
RERANKERS_MODEL_NAME=ms-marco-MiniLM-L-12-v2
RERANKERS_MODEL_TYPE=flashrank

# Bolagsverket Open Data API
# BOLAGSVERKET_BASE_URL="https://gw.api.bolagsverket.se/vardefulla-datamangder/v1"
# BOLAGSVERKET_SUBSCRIPTION_KEY=""
# BOLAGSVERKET_USE_OAUTH=FALSE
# BOLAGSVERKET_API_KEY=""
# Or OAuth client credentials:
# BOLAGSVERKET_CLIENT_ID=""
# BOLAGSVERKET_CLIENT_SECRET=""
# BOLAGSVERKET_TOKEN_URL=""
# BOLAGSVERKET_SCOPE=""

# Trafikverket Open API
# TRAFIKVERKET_API_KEY=""
# TRAFIKVERKET_SCHEMA_VERSION="1.0"
# TRAFIKVERKET_SCHEMA_VERSION_FALLBACKS="1.1,1.2"


# TTS_SERVICE=local/kokoro for local Kokoro TTS or
# LiteLLM TTS Provider: https://docs.litellm.ai/docs/text_to_speech#supported-providers
TTS_SERVICE=local/kokoro
# Respective TTS Service API
# TTS_SERVICE_API_KEY=
# OPTIONAL: TTS Provider API Base
# TTS_SERVICE_API_BASE=

# STT Service Configuration
# For local Faster-Whisper: local/MODEL_SIZE (tiny, base, small, medium, large-v3)
STT_SERVICE=local/base
# For LiteLLM STT Provider: https://docs.litellm.ai/docs/audio_transcription#supported-providers
# STT_SERVICE=openai/whisper-1
# STT_SERVICE_API_KEY=""
# STT_SERVICE_API_BASE=


# (Optional) Maximum pages limit per user for ETL services (default: `999999999` for unlimited in OSS version)  
PAGES_LIMIT=500


FIRECRAWL_API_KEY=fcr-01J0000000000000000000000

# File Parser Service
ETL_SERVICE=UNSTRUCTURED or LLAMACLOUD or DOCLING
UNSTRUCTURED_API_KEY=Tpu3P0U8iy
LLAMA_CLOUD_API_KEY=llx-nnn

# OPTIONAL: Add these for LangSmith Observability
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=lsv2_pt_.....
LANGSMITH_PROJECT=surfsense

# Uvicorn Server Configuration
# Full documentation for Uvicorn options can be found at: https://www.uvicorn.org/#command-line-options
UVICORN_HOST="0.0.0.0"
UVICORN_PORT=8000
UVICORN_LOG_LEVEL=info

# OPTIONAL: Advanced Uvicorn Options (uncomment to use)
# UVICORN_PROXY_HEADERS=false
# UVICORN_FORWARDED_ALLOW_IPS="127.0.0.1"
# UVICORN_WORKERS=1
# UVICORN_ACCESS_LOG=true
# UVICORN_LOOP="auto"
# UVICORN_HTTP="auto"
# UVICORN_WS="auto"
# UVICORN_LIFESPAN="auto"
# UVICORN_LOG_CONFIG=""
# UVICORN_SERVER_HEADER=true
# UVICORN_DATE_HEADER=true
# UVICORN_LIMIT_CONCURRENCY=
# UVICORN_LIMIT_MAX_REQUESTS=
# UVICORN_TIMEOUT_KEEP_ALIVE=5
# UVICORN_TIMEOUT_NOTIFY=30
# UVICORN_SSL_KEYFILE=""
# UVICORN_SSL_CERTFILE=""
# UVICORN_SSL_KEYFILE_PASSWORD=""
# UVICORN_SSL_VERSION=""
# UVICORN_SSL_CERT_REQS=""
# UVICORN_SSL_CA_CERTS=""
# UVICORN_SSL_CIPHERS=""
# UVICORN_HEADERS=""
# UVICORN_USE_COLORS=true
# UVICORN_UDS=""
# UVICORN_FD=""
# UVICORN_ROOT_PATH=""

# ============================================================================
# LOCAL LLM CONFIGURATION — LM Studio & vLLM
# ============================================================================
#
# ── LM Studio ────────────────────────────────────────────────────────────────
# LM Studio is auto-detected when api_base is http://localhost:1234 (default
# LM Studio port). A null-safe compatibility layer is automatically activated
# to work around strict Jinja chat templates used by some models.
#
# Quick setup in the web UI (Search Space Settings > LLM Configurations):
#   - Provider:         CUSTOM
#   - Custom Provider:  openai
#   - Model Name:       nvidia/nemotron-3-nano  (or any model loaded in LM Studio)
#   - API Base:         http://localhost:1234/v1
#   - API Key:          lm-studio   (any non-empty string)
#   - litellm_params:
#       temperature: 0.6
#       max_tokens: 8192
#
# Or use the example entry with id: -11 in global_llm_config.example.yaml.
#
# ── Think mode (nvidia/nemotron-3-nano, DeepSeek-R1, Qwen3, …) ─────────────
# Toggle thinking in LM Studio's UI (the lightbulb icon next to the model).
# When enabled, the model prefixes its response with <think>…</think> tokens.
# The platform automatically strips these from stored message history so they
# don't accumulate in context or interfere with tool-call parsing. Streaming
# think tokens ARE shown to the user in real-time.
# No additional configuration is required — thinking mode works out of the box.
#
# ── Jinja template error: "Cannot apply filter string to NullValue" ─────────
# This is a bug in some models' bundled Jinja templates (seen with early
# builds of nvidia/nemotron-3-nano in LM Studio). Workaround options:
#   1. Download the model from lmstudio-community — these builds ship with
#      corrected prompt templates.
#   2. In LM Studio → My Models → model settings → Prompt Template:
#      Find every occurrence of `| string` and wrap with a null-guard:
#        Before:  {{ message.content | string }}
#        After:   {% if message.content is not none %}{{ message.content | string }}{% endif %}
# The platform already prevents sending content:null in the request, but the
# Jinja bug is in LM Studio's internal rendering of its own output format.
#
# ── vLLM ─────────────────────────────────────────────────────────────────────
# IMPORTANT: Always use the CUSTOM provider (not OLLAMA) for vLLM.
# The OLLAMA provider sends requests to /api/chat (Ollama's native protocol).
# vLLM serves the OpenAI-compatible /v1/chat/completions endpoint instead.
# Using OLLAMA provider with a vLLM URL will cause protocol mismatches.
#
# Correct setup in the web UI under Search Space Settings > LLM Configurations:
#   - Provider: CUSTOM
#   - Custom Provider: openai
#   - Model Name: <your-model-name, e.g. meta-llama/Meta-Llama-3-8B-Instruct>
#   - API Base: http://localhost:8000/v1
#   - API Key: EMPTY  (or leave blank)
#   - litellm_params:
#       max_tokens: 8192
#       temperature: 0.7
#
# Or in global_llm_config.yaml (see example entry with id: -10 in the example file).
#
# ── vLLM server startup (GPU performance) ───────────────────────────────────
# The default vLLM launch leaves significant GPU capacity unused. Start vLLM
# with the flags below to saturate the GPU and enable continuous batching:
#
# ```bash
# python -m vllm.entrypoints.openai.api_server \
#   --model meta-llama/Meta-Llama-3-8B-Instruct \
#   --port 8000 \
#   --host 0.0.0.0 \
#   --dtype bfloat16 \                  # Use bfloat16 (float32 uses 4x more compute)
#   --gpu-memory-utilization 0.95 \     # Dedicate 95 % of VRAM to KV-cache
#   --max-num-batched-tokens 32768 \    # Tokens processed per scheduler step
#   --max-num-seqs 256 \               # Max concurrent sequences in the batch
#   --enable-chunked-prefill \          # Interleave prefill with decode steps
#   --max-model-len 16384              # Max sequence length (adjust for your GPU)
# ```
#
# For multi-GPU setups add: --tensor-parallel-size <num_gpus>
#
# ── Application-side concurrency ────────────────────────────────────────────
# vLLM's continuous batching only helps when multiple requests arrive
# simultaneously. Increase subagent_max_concurrency in the HITL config below
# from the default of 3 to at least 10 when using a local vLLM server.
# ============================================================================
# OLLAMA/vLLM Local LLM Configuration
# ============================================================================
#
